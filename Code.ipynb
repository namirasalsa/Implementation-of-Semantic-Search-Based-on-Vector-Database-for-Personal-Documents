{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Semantic Search based on Vector Database for Personal Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz \n",
    "import re\n",
    "from pymilvus import MilvusClient, connections, Collection, CollectionSchema, FieldSchema, DataType, utility\n",
    "import numpy as np\n",
    "from docx import Document\n",
    "import tempfile\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check the connection\n",
    "make sure to run milvus on docker first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections.connect(alias=\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "if connections.has_connection(\"default\"):\n",
    "    print(\"Connected!\")\n",
    "else:\n",
    "    print(\"Connection failed!\")\n",
    "\n",
    "client = MilvusClient(uri=\"http://localhost:19530\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Dataset\n",
    "### 3.1 Function to convert .docx format file to .pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_docx_to_pdf(docx_path, output_dir):\n",
    "    try:\n",
    "        doc = Document(docx_path)\n",
    "        pdf_filename = os.path.splitext(os.path.basename(docx_path))[0] + \".pdf\"\n",
    "        pdf_path = os.path.join(output_dir, pdf_filename)\n",
    "\n",
    "        # Create PDF\n",
    "        c = canvas.Canvas(pdf_path, pagesize=letter)\n",
    "        text = c.beginText(50, 750)  # Starting position\n",
    "        text.setFont(\"Helvetica\", 10)\n",
    "\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text.textLine(paragraph.text)\n",
    "\n",
    "        c.drawText(text)\n",
    "        c.save()\n",
    "\n",
    "        print(f\"Converted {docx_path} to {pdf_path}.\")\n",
    "        return pdf_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {docx_path} to PDF: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Function to extract text from PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page_num in range(doc.page_count):\n",
    "            page = doc.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Function to extract metadata from each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(file_path):\n",
    "    metadata = {\"title\": os.path.basename(file_path), \"author\": \"Unknown\", \"subject\": \"General\"}\n",
    "    \n",
    "    try:\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            # Extract metadata from PDF\n",
    "            doc = fitz.open(file_path)\n",
    "            pdf_metadata = doc.metadata\n",
    "            metadata[\"title\"] = pdf_metadata.get(\"title\", metadata[\"title\"])\n",
    "            metadata[\"author\"] = pdf_metadata.get(\"author\", metadata[\"author\"])\n",
    "            metadata[\"subject\"] = pdf_metadata.get(\"subject\", metadata[\"subject\"])\n",
    "        elif file_path.endswith(\".docx\"):\n",
    "            # Extract metadata from DOCX\n",
    "            doc = Document(file_path)\n",
    "            core_properties = doc.core_properties\n",
    "            metadata[\"title\"] = core_properties.title or metadata[\"title\"]\n",
    "            metadata[\"author\"] = core_properties.author or metadata[\"author\"]\n",
    "            metadata[\"subject\"] = core_properties.subject or metadata[\"subject\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metadata from {file_path}: {e}\")\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Function to clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove non-word characters\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def generate_embeddings(text, model):\n",
    "    embeddings = model.encode(text)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Function to store Embeddings and metadata in Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_milvus(client, collection_name, embeddings_list, metadata_list):\n",
    "    try:\n",
    "        # Check\n",
    "        if utility.has_collection(collection_name, using=\"default\"):\n",
    "            print(f\"Collection '{collection_name}' already exists. Dropping and recreating...\")\n",
    "            utility.drop_collection(collection_name, using=\"default\")\n",
    "\n",
    "        # Milvus schema definition\n",
    "        fields = [\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "            FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),  \n",
    "            FieldSchema(name=\"title_embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),  \n",
    "            FieldSchema(name=\"subject_embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),  \n",
    "            FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=256),\n",
    "            FieldSchema(name=\"author\", dtype=DataType.VARCHAR, max_length=256),\n",
    "            FieldSchema(name=\"subject\", dtype=DataType.VARCHAR, max_length=256),\n",
    "        ]\n",
    "        schema = CollectionSchema(fields, description=\"Schema for document embeddings\")\n",
    "\n",
    "        # Create the collection\n",
    "        collection = Collection(name=collection_name, schema=schema)\n",
    "         #  buat collection\n",
    "        if not client.has_collection(collection_name):\n",
    "            collection = Collection(name=collection_name, schema=schema)\n",
    "            print(f\"Collection '{collection_name}' created successfully.\")\n",
    "        else:\n",
    "            collection = Collection(name=collection_name)\n",
    "            print(f\"Collection '{collection_name}' already exists.\")\n",
    "\n",
    "        if \"embedding\" not in [field.name for field in collection.schema.fields]:\n",
    "            print(f\"Field 'embedding' does not exist in the collection schema.\")\n",
    "            return\n",
    "        \n",
    "        \n",
    "        # Index parameters\n",
    "        index_params = {\n",
    "            \"index_type\": \"HNSW\",\n",
    "            \"metric_type\": \"L2\",\n",
    "            \"params\": {\"M\": 16, \"efConstruction\": 200},\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "            print(f\"Index created successfully for collection: '{collection_name}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create an index on collection: {collection_name}. Error: {e}\")\n",
    "\n",
    "        try:\n",
    "            collection.create_index(field_name=\"title_embedding\", index_params=index_params)\n",
    "            print(f\"Index created successfully for 'title_embedding' in collection: '{collection_name}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create an index on 'title_embedding' field in collection: {collection_name}. Error: {e}\")\n",
    "\n",
    "        try:\n",
    "            collection.create_index(field_name=\"subject_embedding\", index_params=index_params)\n",
    "            print(f\"Index created successfully for 'subject_embedding' in collection: '{collection_name}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create an index on 'subject_embedding' field in collection: {collection_name}. Error: {e}\")\n",
    "\n",
    "\n",
    "        # memasukkan data ke collection\n",
    "        embeddings_data = [embedding.flatten().tolist() for embedding in embeddings_list]\n",
    "        title_embeddings = [generate_embeddings(metadata.get('title', ''), model).flatten().tolist() for metadata in metadata_list]\n",
    "        subject_embeddings = [generate_embeddings(metadata.get('subject', ''), model).flatten().tolist() for metadata in metadata_list]\n",
    "        \n",
    "        embeddings_data = [embedding.tolist() for embedding in embeddings_list]\n",
    "        titles = [metadata.get('title', '') for metadata in metadata_list]\n",
    "        authors = [metadata.get('author', '') for metadata in metadata_list]\n",
    "        subjects = [metadata.get('subject', '') for metadata in metadata_list]\n",
    "\n",
    "        data = [\n",
    "            embeddings_data,\n",
    "            title_embeddings, \n",
    "            subject_embeddings,  \n",
    "            titles,\n",
    "            authors,\n",
    "            subjects,\n",
    "        ]\n",
    "\n",
    "        # Insert data into Milvus\n",
    "        collection.insert(data)\n",
    "        print(f\"Data stored successfully in collection '{collection_name}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error storing in Milvus: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_store_pdf(dataset_dir, collection_name):\n",
    "    embeddings_list = []\n",
    "    metadata_list = []\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_pdf_dir:\n",
    "        for subfolder in os.listdir(dataset_dir):\n",
    "            subfolder_path = os.path.join(dataset_dir, subfolder)\n",
    "            if os.path.isdir(subfolder_path):\n",
    "                print(f\"Processing folder: {subfolder_path}\")\n",
    "\n",
    "                for file in os.listdir(subfolder_path):\n",
    "                    file_path = os.path.join(subfolder_path, file)\n",
    "\n",
    "                    if file.endswith(\".pdf\"):\n",
    "                        print(f\"Processing PDF file: {file_path}...\")\n",
    "                        text = extract_text_from_pdf(file_path)\n",
    "                    elif file.endswith(\".docx\"):\n",
    "                        print(f\"Converting DOCX to PDF: {file_path}...\")\n",
    "                        converted_pdf_path = convert_docx_to_pdf(file_path, temp_pdf_dir)\n",
    "                        if converted_pdf_path:\n",
    "                            text = extract_text_from_pdf(converted_pdf_path)\n",
    "                        else:\n",
    "                            print(f\"Skipping conversion for {file_path} due to an error.\")\n",
    "                            continue\n",
    "                    else:\n",
    "                        print(f\"Skipping unsupported file: {file_path}\")\n",
    "                        continue\n",
    "\n",
    "                    if text:\n",
    "                        cleaned_text = clean_text(text)\n",
    "                        embeddings = generate_embeddings(cleaned_text, model)\n",
    "                        metadata = extract_metadata(file_path)  \n",
    "                        embeddings_list.append(embeddings)\n",
    "                        metadata_list.append(metadata)\n",
    "\n",
    "        if embeddings_list:\n",
    "            store_in_milvus(client, collection_name, embeddings_list, metadata_list)\n",
    "\n",
    "\n",
    "dataset_dir = \"PersonalDocumentsDataset\" # Adjust the directory\n",
    "collection_name = \"dataset\" \n",
    "\n",
    "process_and_store_pdf(dataset_dir, collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Collection Content\n",
    "Ensure the entire dataset is stored in Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek isi koleksi\n",
    "def check_collection_contents(collection_name):\n",
    "    try:\n",
    "        collection = Collection(name=collection_name)\n",
    "        collection.load()\n",
    "        results = collection.query(expr=\"id >= 0\", output_fields=[\"id\", \"title\", \"author\", \"subject\"], limit=200)\n",
    "\n",
    "        for result in results:\n",
    "            print(f\"Document ID: {result['id']}, Title: {result.get('title', 'N/A')}, Author: {result.get('author', 'N/A')}, Subject: {result.get('subject', 'N/A')}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking collection contents: {e}\")\n",
    "\n",
    "collection_name = \"dataset\"\n",
    "check_collection_contents(collection_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementation of vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(collection_name, query_embedding):\n",
    "    print(\"\\n--- Scenario 1: Direct Vector Search ---\")\n",
    "    try:\n",
    "        search_params = {\n",
    "            \"metric_type\": \"L2\",\n",
    "            \"params\": {\"ef\": 128}\n",
    "        }\n",
    "\n",
    "        collection = Collection(name=collection_name)\n",
    "        collection.load()\n",
    "\n",
    "        # Search on the document embedding\n",
    "        results_doc = collection.search(\n",
    "            data=[query_embedding],\n",
    "            anns_field=\"embedding\",  \n",
    "            param=search_params,\n",
    "            limit=5,\n",
    "            output_fields=[\"title\", \"author\"]\n",
    "        )\n",
    "        \n",
    "        # Search on the title embedding\n",
    "        results_title = collection.search(\n",
    "            data=[query_embedding],\n",
    "            anns_field=\"title_embedding\",\n",
    "            param=search_params,\n",
    "            limit=5,\n",
    "            output_fields=[\"title\", \"author\"]\n",
    "        )\n",
    "\n",
    "        # Search on the subject embedding\n",
    "        results_subject = collection.search(\n",
    "            data=[query_embedding],\n",
    "            anns_field=\"subject_embedding\", \n",
    "            param=search_params,\n",
    "            limit=5,\n",
    "            output_fields=[\"title\", \"author\"]\n",
    "        )\n",
    "\n",
    "        combined_results = []\n",
    "        \n",
    "        # Add results from document search\n",
    "        for hits in results_doc:\n",
    "            for result in hits:\n",
    "                combined_results.append({\"id\": result.id, \"distance\": result.distance, \"type\": \"document\", \"title\": result.title, \"author\": result.author})\n",
    "        \n",
    "        # Add results from title search\n",
    "        for hits in results_title:\n",
    "            for result in hits:\n",
    "                combined_results.append({\"id\": result.id, \"distance\": result.distance, \"type\": \"title\", \"title\": result.title, \"author\": result.author})\n",
    "        \n",
    "        # Add results from subject search\n",
    "        for hits in results_subject:\n",
    "            for result in hits:\n",
    "                combined_results.append({\"id\": result.id, \"distance\": result.distance, \"type\": \"subject\", \"title\": result.title, \"author\": result.author})\n",
    "\n",
    "        # Sort combined results by distance (ascending)\n",
    "        combined_results = sorted(combined_results, key=lambda x: x['distance'])\n",
    "\n",
    "        # Set to track unique document IDs and avoid duplicates\n",
    "        seen_ids = set()\n",
    "        final_results = []\n",
    "        idx = 0\n",
    "\n",
    "        # Loop to collect unique top 5 documents or you can customize as needed\n",
    "        while len(final_results) < 5 and idx < len(combined_results):\n",
    "            result = combined_results[idx]\n",
    "            if result['id'] not in seen_ids:\n",
    "                final_results.append(result)\n",
    "                seen_ids.add(result['id'])\n",
    "            idx += 1\n",
    "            \n",
    "        \n",
    "        # Print the top 5 results (after ensuring uniqueness)\n",
    "        print(\"Top 5 documents based on distance:\")\n",
    "        for result in final_results:\n",
    "            print(f\"Document ID: {result['id']}, Distance: {result['distance']}, Type: {result['type']}\")\n",
    "            print(f\"Title: {result['title']}, Author: {result['author']}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during direct vector search: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pre-processing of user query\n",
    "Function to fix typo and translate user query using Qwen2.5-72B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradio_client import Client\n",
    "\n",
    "def chatbot(query):\n",
    "\tclient = Client(\"Qwen/Qwen2.5-72B-Instruct\")\n",
    "\tresult = client.predict(\n",
    "\t\tquery=query,\n",
    "\t\thistory=[],\n",
    "\t\tsystem=\"anda akan menerima inputan user. cukup tuliskan ulang query tersebut dan perbaiki typonya jika ada. tidak perlu penjelasan apa saja yang diperbaiki. jika input dalam bahasa inggris, translate ke bahasa indonesia.\",\n",
    "\t\tapi_name=\"/model_chat\"\n",
    "\t)\n",
    "\treturn result[1][0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Processing user query and executing vector searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_query_search(collection_name):\n",
    "    # Input\n",
    "    query_text = input(\"Masukkan query pencarian Anda: \")  \n",
    "    \n",
    "    processed_query = chatbot(query_text)\n",
    "    print(f\"Apakah yang anda maksud: {processed_query}\")\n",
    "\n",
    "    query_embedding = generate_embeddings(processed_query, model)\n",
    "    \n",
    "    collection_name =  \"dataset\"\n",
    "\n",
    "    # RUN\n",
    "    vector_search(collection_name=collection_name,  query_embedding=query_embedding.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memanggil fungsi untuk query pengguna \n",
    "user_query_search(collection_name=\"dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
